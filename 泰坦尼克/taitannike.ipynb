{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n"
     ]
    }
   ],
   "source": [
    "import pandas #ipython notebook\n",
    "titanic = pandas.read_csv(\"titanic_train.csv\")\n",
    "# print(titanic.head(3))\n",
    "print(titanic.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  891.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.361582    0.523008   \n",
      "std     257.353842    0.486592    0.836071   13.019697    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   22.000000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   35.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n"
     ]
    }
   ],
   "source": [
    "titanic[\"Age\"] = titanic[\"Age\"].fillna(titanic[\"Age\"].median())\n",
    "print(titanic.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male' 'female']\n"
     ]
    }
   ],
   "source": [
    "print(titanic[\"Sex\"].unique())\n",
    "\n",
    "# Replace all the occurences of male with the number 0.\n",
    "titanic.loc[titanic[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
    "titanic.loc[titanic[\"Sex\"] == \"female\", \"Sex\"] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S' 'C' 'Q' nan]\n"
     ]
    }
   ],
   "source": [
    "print(titanic[\"Embarked\"].unique())\n",
    "titanic[\"Embarked\"] = titanic[\"Embarked\"].fillna('S')\n",
    "titanic.loc[titanic[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "titanic.loc[titanic[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "titanic.loc[titanic[\"Embarked\"] == \"Q\", \"Embarked\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 12)\n",
      "(594, 7)\n",
      "(594,)\n",
      "<class 'numpy.ndarray'>\n",
      "(297,)\n",
      "1\n",
      "<class 'list'>\n",
      "(594, 7)\n",
      "(594,)\n",
      "<class 'numpy.ndarray'>\n",
      "(297,)\n",
      "2\n",
      "<class 'list'>\n",
      "(594, 7)\n",
      "(594,)\n",
      "<class 'numpy.ndarray'>\n",
      "(297,)\n",
      "3\n",
      "<class 'list'>\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Import the linear regression class\n",
    "from sklearn.linear_model import LinearRegression  # 这是线性回归\n",
    "# Sklearn also has a helper that makes it easy to do cross validation\n",
    "# from sklearn.cross_validation import KFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# The columns we'll use to predict the target\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize our algorithm class\n",
    "alg = LinearRegression()\n",
    "# Generate cross validation folds for the titanic dataset.  It return the row indices corresponding to train and test.\n",
    "# We set random_state to ensure we get the same splits every time we run this.\n",
    "print(titanic.shape)\n",
    "# kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "kf = KFold(n_splits=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf.split(titanic):  # titanic.shape[0]  单例数组array（891）不能被视为有效集合\n",
    "    # The predictors we're using the train the algorithm.  Note how we only take the rows in the train folds.\n",
    "    train_predictors = (titanic[predictors].iloc[train,:])\n",
    "    print(train_predictors.shape)\n",
    "    # The target we're using to train the algorithm.\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    print(train_target.shape)\n",
    "    # Training the algorithm using the predictors and target.\n",
    "    # .fit就是把刚才选择的算法应用到我们的数据上，在当前的数据上去训练我们的回归模型\n",
    "    alg.fit(train_predictors, train_target)\n",
    "    # We can now make predictions on the test fold\n",
    "    # print(titanic[predictors].iloc[test,:])\n",
    "    test_predictions = alg.predict(titanic[predictors].iloc[test,:])\n",
    "    print(type(test_predictions))\n",
    "    print(test_predictions.shape)\n",
    "    predictions.append(test_predictions)\n",
    "    print(len(predictions))\n",
    "    print(type(predictions))\n",
    "print(len(predictions))\n",
    "# print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "(891,)\n",
      "[0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1.\n",
      " 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0.\n",
      " 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0.\n",
      " 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.\n",
      " 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0.\n",
      " 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 1. 0.]\n",
      "0.7833894500561167\n"
     ]
    }
   ],
   "source": [
    "print(type(predictions))\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# The predictions are in three separate numpy arrays.  Concatenate them into one.  \n",
    "# We concatenate them on axis 0, as they only have one axis.\n",
    "# numpy提供了numpy.concatenate((a1,a2,...), axis=0)函数。能够一次完成多个数组的拼接。其中a1,a2,...是数组类型的参数\n",
    "predictions = np.concatenate(predictions, axis=0)  # #axis=1表示对应行的数组进行拼接\n",
    "print(type(predictions))\n",
    "print(predictions.shape)\n",
    "\n",
    "# Map predictions to outcomes (only possible outcomes are 1 and 0)\n",
    "predictions[predictions > 0.5] = 1\n",
    "predictions[predictions <= 0.5] = 0\n",
    "# accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print(predictions[predictions == titanic[\"Survived\"]])\n",
    "accuracy = len(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7878787878787877\n"
     ]
    }
   ],
   "source": [
    "# from sklearn import cross_validation\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression  # 逻辑回归，多用来作分类\n",
    "# Initialize our algorithm\n",
    "alg = LogisticRegression(random_state=1, solver = 'liblinear')\n",
    "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
    "# scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "scores = model_selection.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "# cross_val_score(model_name, X, y，cv=k)\n",
    "# 作用：验证某个模型在某个训练集上的稳定性，输出k个预测精度\n",
    "# K折交叉验证（k-fold）把初始训练样本分成k份，其中（k-1）份被用作训练集，剩下一份被用作评估集，这样一共可以对分类器做k次训练，并且得到k个训练结果\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "# 取分数的平均值（因为每一折我们都有一个）\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titanic_test = pandas.read_csv(\"test.csv\")\n",
    "titanic_test = pandas.read_csv(\"titanic_train.csv\")\n",
    "titanic_test[\"Age\"] = titanic_test[\"Age\"].fillna(titanic[\"Age\"].median())\n",
    "titanic_test[\"Fare\"] = titanic_test[\"Fare\"].fillna(titanic_test[\"Fare\"].median())\n",
    "titanic_test.loc[titanic_test[\"Sex\"] == \"male\", \"Sex\"] = 0 \n",
    "titanic_test.loc[titanic_test[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "titanic_test[\"Embarked\"] = titanic_test[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"Q\", \"Embarked\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7856341189674523\n"
     ]
    }
   ],
   "source": [
    "# from sklearn import cross_validation\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize our algorithm with the default paramters\n",
    "# n_estimators is the number of trees we want to make\n",
    "# min_samples_split is the minimum number of rows we need to make a split\n",
    "# min_samples_leaf is the minimum number of samples we can have at the place where a tree branch ends (the bottom points of the tree)\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=2, min_samples_leaf=1)\n",
    "# n_estimators=10 要用多少棵树来构造森林\n",
    "# 树的节点分裂停止的条件\n",
    "# min_samples_split=2  少于样本指定的参数就不再分了\n",
    "# min_samples_leaf=1  最小的叶子节点的个数 \n",
    "\n",
    "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
    "# kf = cross_validation.KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "kf = model_selection.KFold(n_splits=3, random_state=1)\n",
    "scores = model_selection.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=kf.split(titanic))\n",
    "# cross_val_score(model_name, X, y，cv=k)\n",
    "# 作用：验证某个模型在某个训练集上的稳定性，输出k个预测精度\n",
    "\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8148148148148148\n"
     ]
    }
   ],
   "source": [
    "alg = RandomForestClassifier(random_state=1, n_estimators=100, min_samples_split=4, min_samples_leaf=2)\n",
    "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
    "# kf = cross_validation.KFold(titanic.shape[0], 3, random_state=1)\n",
    "kf = model_selection.KFold(n_splits=3, random_state=1)\n",
    "scores = model_selection.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=kf.split(titanic))\n",
    "\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a familysize column\n",
    "titanic[\"FamilySize\"] = titanic[\"SibSp\"] + titanic[\"Parch\"]\n",
    "\n",
    "# The .apply method generates a new series\n",
    "titanic[\"NameLength\"] = titanic[\"Name\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr          517\n",
      "Miss        182\n",
      "Mrs         125\n",
      "Master       40\n",
      "Dr            7\n",
      "Rev           6\n",
      "Mlle          2\n",
      "Major         2\n",
      "Col           2\n",
      "Capt          1\n",
      "Don           1\n",
      "Sir           1\n",
      "Mme           1\n",
      "Countess      1\n",
      "Ms            1\n",
      "Lady          1\n",
      "Jonkheer      1\n",
      "Name: Name, dtype: int64\n",
      "1     517\n",
      "2     183\n",
      "3     125\n",
      "4      40\n",
      "5       7\n",
      "6       6\n",
      "7       5\n",
      "10      3\n",
      "8       3\n",
      "9       2\n",
      "Name: Name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# A function to get the title from a name.\n",
    "def get_title(name):\n",
    "    # Use a regular expression to search for a title.  Titles always consist of capital and lowercase letters, and end with a period.\n",
    "    # 使用正则表达式搜索标题。 标题始终由大写字母和小写字母组成，并以句点结尾。\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    # 如果标题存在，请提取并返回。\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "# Get all the titles and print how often each one occurs.\n",
    "# 获取所有标题并打印每个标题出现的频率\n",
    "titles = titanic[\"Name\"].apply(get_title)\n",
    "print(pandas.value_counts(titles))\n",
    "\n",
    "# Map each title to an integer.  Some titles are very rare, and are compressed into the same codes as other titles.\n",
    "# 将每个标题映射到一个整数。 一些标题非常罕见，并且压缩成与其他标题相同的代码\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "\n",
    "# Verify that we converted everything.\n",
    "# 验证我们是否已转换所有内容\n",
    "print(pandas.value_counts(titles))\n",
    "\n",
    "# Add in the title column.\n",
    "titanic[\"Title\"] = titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_classif  # 根据k个最高分数选择功能\n",
    "import matplotlib.pyplot as plt\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"NameLength\"]\n",
    "\n",
    "# Perform feature selection\n",
    "# 执行功能选择\n",
    "# score_func : callable，函数取两个数组X和y，返回一对数组（scores, pvalues）或一个分数的数组。默认函数为f_classif，默认函数只适用于分类函数\n",
    "# k：int or \"all\", optional, default=10。所选择的topK个特征。“all”选项则绕过选择，用于参数搜索\n",
    "# f_classif 标签/功能之间的ANOVA F值用于分类任务\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selector.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Get the raw p-values for each feature, and transform from p-values into scores\n",
    "# 获取每个特征的原始p值，然后从p值转换为分数\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "# Plot the scores.  See how \"Pclass\", \"Sex\", \"Title\", and \"Fare\" are the best?\n",
    "# 绘制分数。 看看“ Pclass”，“ Sex”，“ Title”和“ Fare”如何最好？\n",
    "plt.bar(range(len(predictors)), scores)\n",
    "# 竖值条形图  bar(x, height, width=0.8, bottom=None, , align='center', data=None, kwargs*)\n",
    "plt.xticks(range(len(predictors)), predictors, rotation='vertical')  # rotation代表lable显示的旋转角度\n",
    "plt.show()\n",
    "\n",
    "# Pick only the four best features.\n",
    "# predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Title\"]\n",
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Title\", \"NameLength\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=50, min_samples_split=8, min_samples_leaf=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(297, 2)\n",
      "(297,)\n",
      "(297, 2)\n",
      "(297,)\n",
      "(297,)\n",
      "(297,)\n",
      "(297, 2)\n",
      "(297,)\n",
      "(297, 2)\n",
      "(297,)\n",
      "(297,)\n",
      "(297,)\n",
      "(297, 2)\n",
      "(297,)\n",
      "(297, 2)\n",
      "(297,)\n",
      "(297,)\n",
      "(297,)\n",
      "(891,)\n",
      "[0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0.\n",
      " 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1.\n",
      " 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0.\n",
      " 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0.\n",
      " 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0.\n",
      " 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1.\n",
      " 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "0.8237934904601572\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier  # ensemble方法其实就是集成方法\n",
    "# 在sacikit-learn中，GradientBoostingClassifier为GBDT的分类类， 而GradientBoostingRegressor为GBDT的回归类\n",
    "# 首先，我们来看boosting框架相关的重要参数\n",
    "# n_estimators: 也就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，\n",
    "# 又容易过拟合，一般选择一个适中的数值。默认是100。在实际调参的过程中，我们常常将n_estimators和下面介绍的参数learning_rate一起考虑\n",
    "# learning_rate: 即每个弱学习器的权重缩减系数ν，也称作步长\n",
    "# alpha：这个参数只有GradientBoostingRegressor有，当我们使用Huber损失\"huber\"和分位数损失“quantile”时，需要指定分位数的值。默认是0.9，如果噪音点较多，可以适当降低这个分位数的值\n",
    "# loss: 即我们GBDT算法中的损失函数。分类模型和回归模型的损失函数是不一样的\n",
    "import numpy as np\n",
    "\n",
    "# The algorithms we want to ensemble.\n",
    "# 我们要集成的算法\n",
    "# We're using the more linear predictors for the logistic regression, and everything with the gradient boosting classifier.\n",
    "# 我们将更多线性预测变量用于logistic回归，而所有使用梯度提升分类器的变量\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\"]],\n",
    "    [LogisticRegression(random_state=1, solver = 'liblinear'), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "# algorithms = [\n",
    "#     [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), [\"Pclass\", \"Sex\", \"Fare\", \"Title\", \"NameLength\"]],\n",
    "#     [LogisticRegression(random_state=1, solver = 'liblinear'), [\"Pclass\", \"Sex\", \"Fare\", \"Title\", \"NameLength\"]]\n",
    "# ]\n",
    "\n",
    "# Initialize the cross validation folds\n",
    "# kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "kf = KFold(n_splits=3, random_state=1)\n",
    "# 这里的话，说实话这个交叉验证导致了结果更差，因为交叉验证我觉得有一点防止过拟合的意思，每个样本既是训练集又是验证集\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf.split(titanic):\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    full_test_predictions = []\n",
    "    # Make predictions for each algorithm on each fold\n",
    "    # 对每个折页的每种算法进行预测\n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm on the training data.\n",
    "        # 使算法适合训练数据\n",
    "        alg.fit(titanic[predictors].iloc[train,:], train_target)\n",
    "        # Select and predict on the test fold.  \n",
    "        # 选择并预测测试结果\n",
    "        # The .astype(float) is necessary to convert the dataframe to all floats and avoid an sklearn error.\n",
    "        # .astype（float）是将Dataframe转换为所有float并避免sklearn错误所必需的\n",
    "        print(alg.predict_proba(titanic[predictors].iloc[test,:]).shape)\n",
    "        # predict_proba返回的是一个 n 行 k 列的数组， 第 i 行 第 j 列上的数值是模型预测 第 i 个预测样本为某个标签的概率，并且每一行的概率和为1\n",
    "        test_predictions = alg.predict_proba(titanic[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        print(test_predictions.shape)\n",
    "        full_test_predictions.append(test_predictions)\n",
    "    # Use a simple ensembling scheme -- just average the predictions to get the final classification.\n",
    "    # 使用简单的整合方案-只需对预测取平均值即可得出最终分类\n",
    "    # test_predictions = (full_test_predictions[0] + full_test_predictions[1]) / 2\n",
    "    # test_predictions = (full_test_predictions[0] + full_test_predictions[1]) / 2\n",
    "    test_predictions = (full_test_predictions[0] * 4 + full_test_predictions[1]) / 5\n",
    "    print(test_predictions.shape)\n",
    "    # Any value over .5 is assumed to be a 1 prediction, and below .5 is a 0 prediction.\n",
    "    # 大于.5的任何值均假定为1预测，小于.5的任何值为0预测\n",
    "    test_predictions[test_predictions <= .5] = 0\n",
    "    test_predictions[test_predictions > .5] = 1\n",
    "    print(test_predictions.shape)\n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "# Put all the predictions together into one array.\n",
    "# 将所有预测汇总到一个数组中\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "print(predictions.shape)\n",
    "\n",
    "# Compute accuracy by comparing to the training data.\n",
    "# 通过与训练数据进行比较来计算准确性\n",
    "# accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print(predictions[predictions == titanic[\"Survived\"]])\n",
    "accuracy = len(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     517\n",
      "2     183\n",
      "3     125\n",
      "4      40\n",
      "5       7\n",
      "6       6\n",
      "7       5\n",
      "10      3\n",
      "8       3\n",
      "9       2\n",
      "Name: Title, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "titles = titanic_test[\"Name\"].apply(get_title)\n",
    "# We're adding the Dona title to the mapping, because it's in the test set, but not the training set\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2, \"Dona\": 10}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "titanic_test[\"Title\"] = titles\n",
    "# Check the counts of each unique title.\n",
    "print(pandas.value_counts(titanic_test[\"Title\"]))\n",
    "\n",
    "# Now, we add the family size column.\n",
    "titanic_test[\"FamilySize\"] = titanic_test[\"SibSp\"] + titanic_test[\"Parch\"]\n",
    "titanic_test[\"NameLength\"] = titanic[\"Name\"].apply(lambda x: len(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 2)\n",
      "(891,)\n",
      "<class 'list'>\n",
      "1\n",
      "(891, 2)\n",
      "(891,)\n",
      "<class 'list'>\n",
      "2\n",
      "<class 'numpy.ndarray'>\n",
      "(891,)\n",
      "0       True\n",
      "1       True\n",
      "2       True\n",
      "3       True\n",
      "4       True\n",
      "5       True\n",
      "6       True\n",
      "7       True\n",
      "8       True\n",
      "9       True\n",
      "10      True\n",
      "11      True\n",
      "12      True\n",
      "13      True\n",
      "14     False\n",
      "15      True\n",
      "16      True\n",
      "17     False\n",
      "18     False\n",
      "19      True\n",
      "20      True\n",
      "21     False\n",
      "22      True\n",
      "23     False\n",
      "24      True\n",
      "25     False\n",
      "26      True\n",
      "27      True\n",
      "28      True\n",
      "29      True\n",
      "       ...  \n",
      "861     True\n",
      "862     True\n",
      "863     True\n",
      "864     True\n",
      "865     True\n",
      "866     True\n",
      "867     True\n",
      "868     True\n",
      "869     True\n",
      "870     True\n",
      "871     True\n",
      "872     True\n",
      "873     True\n",
      "874     True\n",
      "875     True\n",
      "876     True\n",
      "877     True\n",
      "878     True\n",
      "879     True\n",
      "880     True\n",
      "881     True\n",
      "882    False\n",
      "883     True\n",
      "884     True\n",
      "885     True\n",
      "886     True\n",
      "887     True\n",
      "888     True\n",
      "889    False\n",
      "890     True\n",
      "Name: Survived, Length: 891, dtype: bool\n",
      "[0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0.\n",
      " 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.\n",
      " 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1.\n",
      " 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0.\n",
      " 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0.\n",
      " 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1.\n",
      " 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1.\n",
      " 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
      " 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1.\n",
      " 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "753\n",
      "(753,)\n",
      "258.0\n",
      "<class 'numpy.ndarray'>\n",
      "0.8451178451178452\n"
     ]
    }
   ],
   "source": [
    "# predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\"]\n",
    "# predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Title\", \"NameLength\"]\n",
    "# predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Title\"]\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"NameLength\"]\n",
    "\n",
    "# Boosting 算法善于处理偏差(bias)方差(variance)问题（trade-off）\n",
    "# Boosting 是一个以融合原则工作的序列技术，将一系列的弱学习器进行组合从而提升预测的准确率。模型第t次迭代的结果，是基于前t-1次的结果进行赋权\n",
    "# 机器学习之Gradient Tree Boosting中GBDT-- GradientBoostingClassifier\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), predictors],\n",
    "    [LogisticRegression(random_state=1, solver = 'liblinear'), predictors]\n",
    "]\n",
    "\n",
    "full_predictions = []\n",
    "for alg, predictors in algorithms:\n",
    "    # Fit the algorithm using the full training data.\n",
    "    # 使用完整的训练数据拟合算法\n",
    "    alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "    # Predict using the test dataset.  We have to convert all the columns to floats to avoid an error.\n",
    "    # 使用测试数据集进行预测。 我们必须将所有列都转换为浮点数以避免错误。\n",
    "    print(alg.predict_proba(titanic_test[predictors]).shape)\n",
    "    predictions = alg.predict_proba(titanic_test[predictors].astype(float))[:,1]\n",
    "    print(predictions.shape)\n",
    "    full_predictions.append(predictions)\n",
    "    print(type(full_predictions))\n",
    "    print(len(full_predictions))\n",
    "\n",
    "# The gradient boosting classifier generates better predictions, so we weight it higher.\n",
    "# 梯度提升分类器产生更好的预测，因此我们对其加权更高\n",
    "predictions = (full_predictions[0] * 3 + full_predictions[1]) / 4\n",
    "print(type(predictions))  # 就是说list类型和list类型相加就变成了numpy.ndarray类型了啊\n",
    "print(predictions.shape)\n",
    "# print(predictions[predictions])\n",
    "## 用作索引的数组必须为整数（或布尔值）类型\n",
    "# print(predictions[predictions <= .5])\n",
    "predictions[predictions <= .5] = 0  # 就是根据布尔值类型数组索引赋值，还是没什么问题的\n",
    "predictions[predictions > .5] = 1\n",
    "# predictions[test_predictions > .5] = 1\n",
    "# 布尔索引与维度0上的索引数组不匹配； 维度是891，但相应的布尔维度是297\n",
    "\n",
    "print(predictions == titanic[\"Survived\"])  # 这里获得的会是一个布尔值类型数组\n",
    "print(predictions[predictions == titanic[\"Survived\"]])  # 这应该很清晰了吧，就是根据布尔值类型数组里布尔值为Ture选取predictions里预测正确的\n",
    "print(len(predictions[predictions == titanic[\"Survived\"]]))\n",
    "print(predictions[predictions == titanic[\"Survived\"]].shape)\n",
    "print(sum(predictions[predictions == titanic[\"Survived\"]]))  # 下面列出的输出结果一目了然，用sum相加是错误的\n",
    "print(type(predictions[predictions == titanic[\"Survived\"]]))\n",
    "# accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "# sum不是相加的意思么，就是说预测正确的所有标签相加，但是0这个标签是你加不上的啊，所以少加了预测对的但是预测不能够获救的，当然就不对了啊\n",
    "accuracy = len(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
